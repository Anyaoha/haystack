name: benchmarks

on:
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: eu-central-1

jobs:
  deploy-runner:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - uses: iterative/setup-cml@v1

      - name: AWS authentication
        uses: aws-actions/configure-aws-credentials@5fd3084fc36e372ff1fff382a39b10d03659f355
        with:
          aws-region: ${{ env.AWS_REGION }}
          role-to-assume: ${{ secrets.AWS_CI_ROLE_ARN }}

      - name: deploy
        env:
          repo_token: ${{ secrets.HAYSTACK_BOT_TOKEN }}
        run: |
          cd test/benchmarks
          cml runner launch \
          --cloud aws \
          --cloud-region ${{ env.AWS_REGION }} \
          --cloud-type=p3.2xlarge \
          --labels=cml

  run-benchmarks:
    needs: deploy-runner
    runs-on: [self-hosted, cml]
    container:
      image: docker://iterativeai/cml:0-dvc2-base1-gpu
      options: --gpus all
    services:
      opensearch:
        image: opensearchproject/opensearch:1.3.5
        env:
          discovery.type: "single-node"
        ports:
          - 9200:9200
      elasticsearch:
        image: elasticsearch:7.17.6
        env:
          discovery.type: "single-node"
        ports:
          - 9201:9200
      weaviate:
        image: semitechnologies/weaviate:1.17.2
        env:
          AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"
          PERSISTENCE_DATA_PATH: "/var/lib/weaviate"
        ports:
          - 8080:8080
    timeout-minutes: 2880

    steps:
      - uses: actions/checkout@v3

      - name: Install Haystack
        run: pip install .[metrics,elasticsearch,weaviate,opensearch,benchmarks,inference]

      - name: Run benchmarks
        run: |
          cd test/benchmarks
          mkdir +p out
          for f in ./configs/retriever/bm25-weaviate*.yml; do
            name="${f%.*}"
            echo "=== Running benchmarks for $name ===";
            config_name="$(basename "$name")"
            python run.py --output "out/$config_name.json" "$f";
            echo "=== Benchmarks done for $name (or failed) ===";
          done

      - name: Send Benchmark results to Datadog
        run: |
          python datadog/send_metrics.py
